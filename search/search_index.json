{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GitOps Docs \u00b6 Welcome to the docs on Kubernetes cluster. Work in progress This document is a work in progress.","title":"Introduction"},{"location":"#gitops-docs","text":"Welcome to the docs on Kubernetes cluster. Work in progress This document is a work in progress.","title":"GitOps Docs"},{"location":"external-secrets/","text":"External Secrets \u00b6 Work in progress This document is a work in progress. Create secret for External Secrets using AWS Secrets Manager \u00b6 kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system Create a secret using aws-cli \u00b6 aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"External Secrets"},{"location":"external-secrets/#external-secrets","text":"Work in progress This document is a work in progress.","title":"External Secrets"},{"location":"external-secrets/#create-secret-for-external-secrets-using-aws-secrets-manager","text":"kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system","title":"Create secret for External Secrets using AWS Secrets Manager"},{"location":"external-secrets/#create-a-secret-using-aws-cli","text":"aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"Create a secret using aws-cli"},{"location":"flux/","text":"Flux \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install fluxcd/tap/flux Install the cluster components \u00b6 For full installation guide visit the Flux installation guide Set the GITHUB_TOKEN environment variable to a personal access token you created in Github. export GITHUB_TOKEN = 47241b5a1f9cc45cc7388cf787fc6abacf99eb70 Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster flux bootstrap github \\ --version = v0.9.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work Useful commands \u00b6 Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Flux"},{"location":"flux/#flux","text":"Work in progress This document is a work in progress.","title":"Flux"},{"location":"flux/#install-the-cli-tool","text":"brew install fluxcd/tap/flux","title":"Install the CLI tool"},{"location":"flux/#install-the-cluster-components","text":"For full installation guide visit the Flux installation guide Set the GITHUB_TOKEN environment variable to a personal access token you created in Github. export GITHUB_TOKEN = 47241b5a1f9cc45cc7388cf787fc6abacf99eb70 Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster flux bootstrap github \\ --version = v0.9.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work","title":"Install the cluster components"},{"location":"flux/#useful-commands","text":"Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Useful commands"},{"location":"helmrelease/","text":"HelmRelease \u00b6 Work in progress This document is a work in progress.","title":"HelmRelease"},{"location":"helmrelease/#helmrelease","text":"Work in progress This document is a work in progress.","title":"HelmRelease"},{"location":"rook-ceph-maintenance/","text":"Rook-Ceph Maintenance \u00b6 Work in progress This document is a work in progress. Accessing volumes \u00b6 Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep zigbee2mqtt-data | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.1.40:/volume1/Data /mnt/nfsdata List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home Handling crashes \u00b6 Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all Helpful links \u00b6 Common issues","title":"Rook-Ceph Maintenance"},{"location":"rook-ceph-maintenance/#rook-ceph-maintenance","text":"Work in progress This document is a work in progress.","title":"Rook-Ceph Maintenance"},{"location":"rook-ceph-maintenance/#accessing-volumes","text":"Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep zigbee2mqtt-data | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.1.40:/volume1/Data /mnt/nfsdata List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home","title":"Accessing volumes"},{"location":"rook-ceph-maintenance/#handling-crashes","text":"Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all","title":"Handling crashes"},{"location":"rook-ceph-maintenance/#helpful-links","text":"Common issues","title":"Helpful links"},{"location":"sealed-secrets/","text":"Sealed Secrets \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install kubeseal Install the cluster components \u00b6 --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false Fetch the Sealed Secrets public certificate \u00b6 kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"Sealed Secrets"},{"location":"sealed-secrets/#sealed-secrets","text":"Work in progress This document is a work in progress.","title":"Sealed Secrets"},{"location":"sealed-secrets/#install-the-cli-tool","text":"brew install kubeseal","title":"Install the CLI tool"},{"location":"sealed-secrets/#install-the-cluster-components","text":"--- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false","title":"Install the cluster components"},{"location":"sealed-secrets/#fetch-the-sealed-secrets-public-certificate","text":"kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"Fetch the Sealed Secrets public certificate"},{"location":"snmp-exporter/","text":"SNMP Exporter \u00b6 Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus Clone and build the snmp-exporter generator \u00b6 sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs Update generator.yml \u00b6 Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%) Get the Cyberpower MIB \u00b6 wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/ Generate the snmp.yml \u00b6 This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"Snmp Exporter"},{"location":"snmp-exporter/#snmp-exporter","text":"Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus","title":"SNMP Exporter"},{"location":"snmp-exporter/#clone-and-build-the-snmp-exporter-generator","text":"sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs","title":"Clone and build the snmp-exporter generator"},{"location":"snmp-exporter/#update-generatoryml","text":"Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%)","title":"Update generator.yml"},{"location":"snmp-exporter/#get-the-cyberpower-mib","text":"wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/","title":"Get the Cyberpower MIB"},{"location":"snmp-exporter/#generate-the-snmpyml","text":"This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"Generate the snmp.yml"},{"location":"velero/","text":"Velero \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install velero Create a backup \u00b6 Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait Delete resources \u00b6 Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config Restore \u00b6 velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Velero"},{"location":"velero/#velero","text":"Work in progress This document is a work in progress.","title":"Velero"},{"location":"velero/#install-the-cli-tool","text":"brew install velero","title":"Install the CLI tool"},{"location":"velero/#create-a-backup","text":"Create a backup for all apps: velero backup create manually-backup-1 --from-schedule velero-daily-backup Create a backup for a single app: velero backup create jackett-test-abc \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Create a backup"},{"location":"velero/#delete-resources","text":"Delete the HelmRelease : kubectl delete hr jackett-test -n testing Wait Allow the application to be redeployed and create the new resources Delete the new resources: kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config","title":"Delete resources"},{"location":"velero/#restore","text":"velero restore create \\ --from-backup velero-daily-backup-20201120020022 \\ --include-namespaces testing \\ --selector \"app.kubernetes.io/instance=jackett-test\" \\ --wait","title":"Restore"},{"location":"aws/ec2/","text":"EC2 \u00b6 Work in progress This document is a work in progress.","title":"EC2"},{"location":"aws/ec2/#ec2","text":"Work in progress This document is a work in progress.","title":"EC2"},{"location":"aws/eks/","text":"EKS \u00b6 Work in progress This document is a work in progress.","title":"EKS"},{"location":"aws/eks/#eks","text":"Work in progress This document is a work in progress.","title":"EKS"},{"location":"cache/redis/","text":"Work in progress This document is a work in progress.","title":"Redis"},{"location":"databases/mongodb/","text":"Work in progress This document is a work in progress.","title":"MongoDB"},{"location":"databases/mysql/","text":"Work in progress This document is a work in progress.","title":"MySQL"},{"location":"databases/postgresql/","text":"Work in progress This document is a work in progress.","title":"PostgreSQL"},{"location":"docker/volume/","text":"Volume \u00b6 Work in progress This document is a work in progress.","title":"volume"},{"location":"docker/volume/#volume","text":"Work in progress This document is a work in progress.","title":"Volume"},{"location":"gcp/vps/","text":"Work in progress This document is a work in progress.","title":"VPS"},{"location":"gitops/argo/","text":"Work in progress This document is a work in progress.","title":"Argo"},{"location":"gitops/fluxcd/","text":"Work in progress This document is a work in progress.","title":"Fluxcd"},{"location":"helm/tips/","text":"Work in progress This document is a work in progress.","title":"Tips"},{"location":"k8s/deployment/","text":"deployment \u00b6 Work in progress This document is a work in progress.","title":"deployment"},{"location":"k8s/deployment/#deployment","text":"Work in progress This document is a work in progress.","title":"deployment"},{"location":"k8s/service/","text":"service \u00b6 Work in progress This document is a work in progress.","title":"service"},{"location":"k8s/service/#service","text":"Work in progress This document is a work in progress.","title":"service"},{"location":"networking/iptables/","text":"Work in progress This document is a work in progress.","title":"Iptables"},{"location":"opnsense/bgp/","text":"Opnsense | BGP \u00b6 Work in progress This document is a work in progress.","title":"BGP"},{"location":"opnsense/bgp/#opnsense-bgp","text":"Work in progress This document is a work in progress.","title":"Opnsense | BGP"},{"location":"opnsense/pxe/","text":"Opnsense | PXE \u00b6 Work in progress This document is a work in progress. Setting up TFTP \u00b6 Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"PXE"},{"location":"opnsense/pxe/#opnsense-pxe","text":"Work in progress This document is a work in progress.","title":"Opnsense | PXE"},{"location":"opnsense/pxe/#setting-up-tftp","text":"Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"Setting up TFTP"}]}